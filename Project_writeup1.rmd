---
title: Coursera Project for Practical Machine Learning
output: html_document
---

I first load both the training and test data into R using the read.csv command. The string varaibles in the data are automatically converted to factor variables in the above command by setting stringsAsFactors=TRUE. The only two variables that are strings in the data are user_name and the classe variable. 

```{r}
training <- read.csv("C:/Users/Srijita/Desktop/PracMachineLearningProj/pml-training.csv", stringsAsFactors=TRUE)
test <- read.csv("C:/Users/Srijita/Desktop/PracMachineLearningProj/pml-testing.csv", stringsAsFactors=TRUE)

dim(training)
dim(test)

```


The training set has `r dim(training)[1]` observations and `r dim(training)[2]` varaibles while the test set has `r dim(test)[1]` observations and `r dim(test)[2]` variables (the last variable is the classe variable to predict)


We look at the type of data in the training dataset

```{r}
str(training)

summary(training)
```

Looking as the output of these commands, it seems that there are a lot of NA's or blank 
values in a lot of the variables.Hence, I run a for loop to select only those variables that have at least 75% of non-blank and non-NA data.

```{r}
num_obs = dim(training)[1]
var_names = names(training)

ct=0
selected_vars <-""
for (i in 1:length(var_names)) {
  s1 = sum(is.na(training[,i]))
  s2 = sum(training[,i] == "")
  if(is.na(s2)) s2=0
  
  #take only vars having at lest 75% valid data (i.e excluding blanks and NA)
  if((1- (s1+s2)/num_obs)>.75){
    ct= ct+1
    selected_vars[ct] = var_names[i]
  }
} 


```

Check what variables have been selected after this procedure

```{r}
selected_vars
length(selected_vars)
```

Notice that variable 1 is just an index variable (i.e. number of the observation) and variables in columns 3 to 7  are time stamp variables which are unlikley to have any effect on the classe variable. Hence these variables are also dropped from both the training and test dataset.

So the training and test datasets are recreated by keeping only these variables (except the variables in columns 1 and 3-7) and dropping the rest of the variables.For test data the last column ("problem_id") has a different name to that of the training data, so I drop the last column also from the test data. Anyways, this is the column I have to predict in the test data.

```{r}
training <- training[, selected_vars]
training <- training[,-c(1,3:7)]

len = length(selected_vars)
test <- test[, selected_vars[-len]]
test <- test[,-c(1,3:7)]

dim(training)
dim(test)

```


The training data now has `r dim(training)[1]` observations (as before) and `r dim(training)[2]` columns/variables. So `r 160 - dim(training)[2]` variables have been dropped because 
of poor data quality or because it was felt they were not useful information relating to the classification 
problem. Similarly, the test data has `r dim(test)[1]` observations and `r dim(test)[2]`  columns/variables.


Finally, I check which variables have been retained in the training and test dataset
```{r}
names(training)
names(test)
```


Next I set up a control for the train function to be 10-fold cv (using train's inbuilt resampling technique for computing CV results), using the following commands.

```{r}
library(caret)
#Estimate the out of sample error using a 10-fold cross Validation
fit_control = trainControl(method = "cv", number = 10)

```

## Model fitting

Next I run 4 different classification methods in the following order: Rpart, Qda, GBM and RF (random 
forest). To speed up my calculations, I use the parallel facility inbuilt in the train function.


I have 6 
cores on my computer, so I start a cluster of 6 before using the train function. I set the seed to the same value everytime before the train function is called to ensure that the 10-fold CV generated in the train function picks up the same random split for each of the four different model runs. This simply ensures better comparaility among the model results. 

```{r }

library(doParallel)

cores =6
cl <- makeCluster(cores)
registerDoParallel(cl)

set.seed(123)
model_rpart <- train(training$classe ~., method = "rpart",data = training, trControl = fit_control)

set.seed(123)
model_qda <- train(training$classe ~., method = "qda",data = training, trControl = fit_control)

set.seed(123)
model_gbm <- train(training$classe ~., method = "gbm",data = training, trControl = fit_control)

set.seed(123)
model_rf <- train(training$classe ~., method = "rf",data = training, trControl = fit_control)


stopCluster(cl)

```

Next I look at the final models fitted by each of the four methods and compare the accuracy for each of these methods.

```{r}
model_rpart


model_qda


model_gbm


model_rf



```

The results above show that random forest model gives the highest accuracy. Hence Random forests seem to be the best method in this case.

The estimate of accuracy in the random forest method (using 10-fold CV) is: `r idx = (model_rf$results[,1] == model_rf$bestTune[,]); model_rf$results[idx,2]*100`%. 

Hence I expect the out of sample error rate to be 1- accuracy = `r (1- model_rf$results[idx,2])*100`%



Finally, I use the random forest model to predict the test set.

```{r}
predictions <-predict(model_rf, test)
predictions
```


